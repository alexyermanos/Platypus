}
a <- PlatypusDB_fetch("kreiner2021a//metadata")
a <- PlatypusDB_fetch("kreiner2021a//metadata", load.to.list = T)
View(a)
load(url("https://storage.googleapis.com/platypusdb_data/kreiner2021a__metadata.RData"))
View(kreiner2021a__metadata)
load("C:/Users/vickr/Downloads/kreiner2021a__metadata.gz")
View(kreiner2021a__metadata)
load(url("https://storage.googleapis.com/platypusdb_data/kreiner2021a__metadata.RData"))
View(kreiner2021a__metadata)
load("C:/Users/vickr/Downloads/kreiner2021a__metadata.gz")
View(kreiner2021a__metadata)
#' This function generates new db lookup and user lookup tables from the PlatypusDB. This is normally done within the Platypus_DB_upload function. In case of manual changes to files, incomplete uploads or other changes after the last use of PlatypusDB_upload, this function may be used to generate up-to-date lookup tables. To use, you will need writing permission from this R session
#'@param bucket.name Character. Name of bucket containing data in. Defaults to "platypusdb_data" (This is a feature to support future changes to the database)
#' @return Writes a .csv file to the current working directory, which contains the lookup table for the website. The lookup for the R interface will be uploaded to PlatypusDB and returned for control
#' @export
#' @examples
#' \dontrun{
#' new_table <- PlatypusDB_refresh_lookup(bucket.name = "bucket with database data")
#' }
#'
PlatypusDB_refresh_lookup <- function(bucket.name){
platypusdb_lookup <- NULL
project.id <- NULL
if(missing(bucket.name)) bucket.name <- "platypusdb_data"
print("--------------")
print("Getting updated DB lookup list")
#get an updated list of the projects with direct download links (main db lookup table)
db.lookup <- googleCloudStorageR::gcs_list_objects(bucket = bucket.name)
db.lookup$project_id <- stringr::str_split(db.lookup$name, "_", simplify = T)[,1]
db.lookup$sample_id <- stringr::str_split(db.lookup$name, "_", simplify = T)[,2]
db.lookup$filetype <- stringr::str_split(db.lookup$name, "_", simplify = T)[,3]
db.lookup$url <- "none"
for(j in 1:nrow(db.lookup)){
db.lookup$url[j] <- paste0("https://storage.googleapis.com/",bucket.name,"/", db.lookup$name[j]) #append url download info to each entry
}
for(k in 1:ncol(db.lookup)){
db.lookup[,k] <- as.character(db.lookup[,k])
}
#adjust size column to make everything in bytes
size_nb <- as.numeric(stringr::str_split(db.lookup$size, " ", simplify = T)[,1])
size_id <- stringr::str_split(db.lookup$size, " ", simplify = T)[,2]
size_id <- gsub("bytes","1", size_id)
size_id <- gsub("Kb","1000", size_id)
size_id <- gsub("Mb","1000000", size_id)
size_id <- gsub("Gb","1000000000", size_id)
size_id <- as.numeric(size_id)
size_nb <- size_nb * size_id
db.lookup$size <- size_nb
platypus_url_lookup <- db.lookup
print("--------------")
print("Updating R interface DB lookup list in platpyusdb_lookup bucket")
googleCloudStorageR::gcs_save(platypus_url_lookup,file = "platypus_url_lookup.RData", bucket = "platypusdb_lookup")
print("--------------")
print("Getting updated user lookup table for website")
user.lookup.list <- list()
for(i in 1:length(unique(db.lookup$project_id))){
cur.db.lookup <- subset(db.lookup, project_id == unique(db.lookup$project_id)[i])
print(paste0("Processing project ", i , " of ", length(unique(db.lookup$project_id))))
cols <- c("Project ID", "Title", "Organism", "Celltypes", "n","Authors", "DOI", "Journal", "Date", "VDJ_Raw.zip","GEX_Raw.zip")
user.lookup.cur <- stats::setNames(data.frame(matrix(ncol = length(cols), nrow = 1)), cols)
#load in the metadata frame
url_ <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__metadata.RData")
load(url(url_), envir = .GlobalEnv)
cur_meta <- get(paste0(cur.db.lookup$project_id[1], "__metadata"))
rm(list = ls(pattern = paste0(cur.db.lookup$project_id[1], "__metadata")))
#fill in the content (parser for the metadata file)
cur_meta <- cur_meta[c(1:2),] #We only need the two first rows
user.lookup.cur[,"Project ID"] <- cur.db.lookup$project_id[1]
user.lookup.cur[,"Title"] <- cur_meta[2,1]
user.lookup.cur[,"Organism"] <- cur_meta[2,2]
user.lookup.cur[,"Celltypes"] <- cur_meta[2,3]
user.lookup.cur[,"Nr of cells"] <- cur_meta[2,4]
user.lookup.cur[,"Authors"] <- cur_meta[2,5]
user.lookup.cur[,"DOI"] <- cur_meta[2,6]
user.lookup.cur[,"Journal"] <- cur_meta[2,7]
user.lookup.cur[,"Date"] <- cur_meta[2,8]
user.lookup.cur[,"VDJ_Raw.zip"] <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__VDJ_RAW.zip")
user.lookup.cur[,"GEX_Raw.zip"] <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__GEX_RAW.zip")
user.lookup.list[[i]] <- user.lookup.cur
}
print("--------------")
print("Saving PlatypusDB_content_table.csv to the current working directory. Please push this to Githup to update the PlatypusDB website")
user.lookup.list <- as.data.frame(dplyr::bind_rows(user.lookup.list))
utils::write.csv(user.lookup.list, file = "PlatypusDB_content_table.csv", row.names = F)
print(paste0(Sys.time()," Done"))
return(list("Lookup for R interface" = platypus_url_lookup, "User lookup for website" = user.lookup.list))
}
#
PlatypusDB_refresh_lookup()
#' This function generates new db lookup and user lookup tables from the PlatypusDB. This is normally done within the Platypus_DB_upload function. In case of manual changes to files, incomplete uploads or other changes after the last use of PlatypusDB_upload, this function may be used to generate up-to-date lookup tables. To use, you will need writing permission from this R session
#'@param bucket.name Character. Name of bucket containing data in. Defaults to "platypusdb_data" (This is a feature to support future changes to the database)
#' @return Writes a .csv file to the current working directory, which contains the lookup table for the website. The lookup for the R interface will be uploaded to PlatypusDB and returned for control
#' @export
#' @examples
#' \dontrun{
#' new_table <- PlatypusDB_refresh_lookup(bucket.name = "bucket with database data")
#' }
#'
PlatypusDB_refresh_lookup <- function(bucket.name){
platypusdb_lookup <- NULL
project.id <- NULL
if(missing(bucket.name)) bucket.name <- "platypusdb_data"
print("--------------")
print("Getting updated DB lookup list")
#get an updated list of the projects with direct download links (main db lookup table)
db.lookup <- googleCloudStorageR::gcs_list_objects(bucket = bucket.name)
db.lookup$project_id <- stringr::str_split(db.lookup$name, "_", simplify = T)[,1]
db.lookup$sample_id <- stringr::str_split(db.lookup$name, "_", simplify = T)[,2]
db.lookup$filetype <- stringr::str_split(db.lookup$name, "_", simplify = T)[,3]
db.lookup$url <- "none"
for(j in 1:nrow(db.lookup)){
db.lookup$url[j] <- paste0("https://storage.googleapis.com/",bucket.name,"/", db.lookup$name[j]) #append url download info to each entry
}
for(k in 1:ncol(db.lookup)){
db.lookup[,k] <- as.character(db.lookup[,k])
}
#adjust size column to make everything in bytes
size_nb <- as.numeric(stringr::str_split(db.lookup$size, " ", simplify = T)[,1])
size_id <- stringr::str_split(db.lookup$size, " ", simplify = T)[,2]
size_id <- gsub("bytes","1", size_id)
size_id <- gsub("Kb","1000", size_id)
size_id <- gsub("Mb","1000000", size_id)
size_id <- gsub("Gb","1000000000", size_id)
size_id <- as.numeric(size_id)
size_nb <- size_nb * size_id
db.lookup$size <- size_nb
platypus_url_lookup <- db.lookup
print("--------------")
print("Updating R interface DB lookup list in platpyusdb_lookup bucket")
googleCloudStorageR::gcs_save(platypus_url_lookup,file = "platypus_url_lookup.RData", bucket = "platypusdb_lookup")
print("--------------")
print("Getting updated user lookup table for website")
user.lookup.list <- list()
for(i in 1:length(unique(db.lookup$project_id))){
cur.db.lookup <- subset(db.lookup, project_id == unique(db.lookup$project_id)[i])
print(paste0("Processing project ", i , " of ", length(unique(db.lookup$project_id))))
cols <- c("Project ID", "Title", "Organism", "Celltypes", "n","Authors", "DOI", "Journal", "Date", "VDJ_Raw.zip","GEX_Raw.zip")
user.lookup.cur <- stats::setNames(data.frame(matrix(ncol = length(cols), nrow = 1)), cols)
#load in the metadata frame
url_ <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__metadata.RData")
load(url(url_), envir = .GlobalEnv)
cur_meta <- get(paste0(cur.db.lookup$project_id[1], "__metadata"))
rm(list = ls(pattern = paste0(cur.db.lookup$project_id[1], "__metadata")), envir = .GlobalEnv)
#fill in the content (parser for the metadata file)
cur_meta <- cur_meta[c(1:2),] #We only need the two first rows
user.lookup.cur[,"Project ID"] <- cur.db.lookup$project_id[1]
user.lookup.cur[,"Title"] <- cur_meta[2,1]
user.lookup.cur[,"Organism"] <- cur_meta[2,2]
user.lookup.cur[,"Celltypes"] <- cur_meta[2,3]
user.lookup.cur[,"n"] <- cur_meta[2,4]
user.lookup.cur[,"Authors"] <- cur_meta[2,5]
user.lookup.cur[,"Date"] <- cur_meta[2,6]
user.lookup.cur[,"Journal"] <- cur_meta[2,7]
user.lookup.cur[,"DOI"] <- cur_meta[2,8]
user.lookup.cur[,"VDJ_Raw.zip"] <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__VDJ_RAW.zip")
user.lookup.cur[,"GEX_Raw.zip"] <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__GEX_RAW.zip")
user.lookup.list[[i]] <- user.lookup.cur
}
print("--------------")
print("Saving PlatypusDB_content_table.csv to the current working directory. Please push this to Githup to update the PlatypusDB website")
user.lookup.list <- as.data.frame(dplyr::bind_rows(user.lookup.list))
utils::write.csv(user.lookup.list, file = "PlatypusDB_content_table.csv", row.names = F)
print(paste0(Sys.time()," Done"))
return(list("Lookup for R interface" = platypus_url_lookup, "User lookup for website" = user.lookup.list))
}
#' This function generates new db lookup and user lookup tables from the PlatypusDB. This is normally done within the Platypus_DB_upload function. In case of manual changes to files, incomplete uploads or other changes after the last use of PlatypusDB_upload, this function may be used to generate up-to-date lookup tables. To use, you will need writing permission from this R session
#'@param bucket.name Character. Name of bucket containing data in. Defaults to "platypusdb_data" (This is a feature to support future changes to the database)
#' @return Writes a .csv file to the current working directory, which contains the lookup table for the website. The lookup for the R interface will be uploaded to PlatypusDB and returned for control
#' @export
#' @examples
#' \dontrun{
#' new_table <- PlatypusDB_refresh_lookup(bucket.name = "bucket with database data")
#' }
#'
PlatypusDB_refresh_lookup <- function(bucket.name){
platypusdb_lookup <- NULL
project.id <- NULL
if(missing(bucket.name)) bucket.name <- "platypusdb_data"
print("--------------")
print("Getting updated DB lookup list")
#get an updated list of the projects with direct download links (main db lookup table)
db.lookup <- googleCloudStorageR::gcs_list_objects(bucket = bucket.name)
db.lookup$project_id <- stringr::str_split(db.lookup$name, "_", simplify = T)[,1]
db.lookup$sample_id <- stringr::str_split(db.lookup$name, "_", simplify = T)[,2]
db.lookup$filetype <- stringr::str_split(db.lookup$name, "_", simplify = T)[,3]
db.lookup$url <- "none"
for(j in 1:nrow(db.lookup)){
db.lookup$url[j] <- paste0("https://storage.googleapis.com/",bucket.name,"/", db.lookup$name[j]) #append url download info to each entry
}
for(k in 1:ncol(db.lookup)){
db.lookup[,k] <- as.character(db.lookup[,k])
}
#adjust size column to make everything in bytes
size_nb <- as.numeric(stringr::str_split(db.lookup$size, " ", simplify = T)[,1])
size_id <- stringr::str_split(db.lookup$size, " ", simplify = T)[,2]
size_id <- gsub("bytes","1", size_id)
size_id <- gsub("Kb","1000", size_id)
size_id <- gsub("Mb","1000000", size_id)
size_id <- gsub("Gb","1000000000", size_id)
size_id <- as.numeric(size_id)
size_nb <- size_nb * size_id
db.lookup$size <- size_nb
platypus_url_lookup <- db.lookup
print("--------------")
print("Updating R interface DB lookup list in platpyusdb_lookup bucket")
googleCloudStorageR::gcs_save(platypus_url_lookup,file = "platypus_url_lookup.RData", bucket = "platypusdb_lookup")
print("--------------")
print("Getting updated user lookup table for website")
user.lookup.list <- list()
for(i in 1:length(unique(db.lookup$project_id))){
cur.db.lookup <- subset(db.lookup, project_id == unique(db.lookup$project_id)[i])
print(paste0("Processing project ", i , " of ", length(unique(db.lookup$project_id))))
cols <- c("Project ID", "Title", "Organism", "Celltypes", "n","Authors", "DOI", "Journal", "Date", "VDJ_Raw.zip","GEX_Raw.zip")
user.lookup.cur <- stats::setNames(data.frame(matrix(ncol = length(cols), nrow = 1)), cols)
#load in the metadata frame
url_ <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__metadata.RData")
load(url(url_), envir = .GlobalEnv)
cur_meta <- get(paste0(cur.db.lookup$project_id[1], "__metadata"))
rm(list = ls(pattern = paste0(cur.db.lookup$project_id[1], "__metadata")), envir = .GlobalEnv)
#fill in the content (parser for the metadata file)
cur_meta <- cur_meta[c(1:2),] #We only need the two first rows
user.lookup.cur[,"Project ID"] <- cur.db.lookup$project_id[1]
user.lookup.cur[,"Title"] <- cur_meta[2,1]
user.lookup.cur[,"Organism"] <- cur_meta[2,2]
user.lookup.cur[,"Celltypes"] <- cur_meta[2,3]
user.lookup.cur[,"n"] <- cur_meta[2,4]
user.lookup.cur[,"Authors"] <- cur_meta[2,5]
user.lookup.cur[,"Date"] <- cur_meta[2,6]
user.lookup.cur[,"Journal"] <- cur_meta[2,7]
user.lookup.cur[,"DOI"] <- cur_meta[2,8]
user.lookup.cur[,"VDJ_Raw.zip"] <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__VDJ_RAW.zip")
user.lookup.cur[,"GEX_Raw.zip"] <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__GEX_RAW.zip")
user.lookup.list[[i]] <- user.lookup.cur
}
print("--------------")
print("Saving PlatypusDB_content_table.csv to the current working directory. Please push this to Githup to update the PlatypusDB website")
user.lookup.list <- as.data.frame(dplyr::bind_rows(user.lookup.list))
utils::write.csv(user.lookup.list, file = "PlatypusDB_content_table.csv", row.names = F)
print(paste0(Sys.time()," Done"))
return(list("Lookup for R interface" = platypus_url_lookup, "User lookup for website" = user.lookup.list))
}
#
PlatypusDB_refresh_lookup()
#' This function generates new db lookup and user lookup tables from the PlatypusDB. This is normally done within the Platypus_DB_upload function. In case of manual changes to files, incomplete uploads or other changes after the last use of PlatypusDB_upload, this function may be used to generate up-to-date lookup tables. To use, you will need writing permission from this R session
#'@param bucket.name Character. Name of bucket containing data in. Defaults to "platypusdb_data" (This is a feature to support future changes to the database)
#' @return Writes a .csv file to the current working directory, which contains the lookup table for the website. The lookup for the R interface will be uploaded to PlatypusDB and returned for control
#' @export
#' @examples
#' \dontrun{
#' new_table <- PlatypusDB_refresh_lookup(bucket.name = "bucket with database data")
#' }
#'
PlatypusDB_refresh_lookup <- function(bucket.name){
platypusdb_lookup <- NULL
project.id <- NULL
if(missing(bucket.name)) bucket.name <- "platypusdb_data"
print("--------------")
print("Getting updated DB lookup list")
#get an updated list of the projects with direct download links (main db lookup table)
db.lookup <- googleCloudStorageR::gcs_list_objects(bucket = bucket.name)
db.lookup$project_id <- stringr::str_split(db.lookup$name, "_", simplify = T)[,1]
db.lookup$sample_id <- stringr::str_split(db.lookup$name, "_", simplify = T)[,2]
db.lookup$filetype <- stringr::str_split(db.lookup$name, "_", simplify = T)[,3]
db.lookup$url <- "none"
for(j in 1:nrow(db.lookup)){
db.lookup$url[j] <- paste0("https://storage.googleapis.com/",bucket.name,"/", db.lookup$name[j]) #append url download info to each entry
}
for(k in 1:ncol(db.lookup)){
db.lookup[,k] <- as.character(db.lookup[,k])
}
#adjust size column to make everything in bytes
size_nb <- as.numeric(stringr::str_split(db.lookup$size, " ", simplify = T)[,1])
size_id <- stringr::str_split(db.lookup$size, " ", simplify = T)[,2]
size_id <- gsub("bytes","1", size_id)
size_id <- gsub("Kb","1000", size_id)
size_id <- gsub("Mb","1000000", size_id)
size_id <- gsub("Gb","1000000000", size_id)
size_id <- as.numeric(size_id)
size_nb <- size_nb * size_id
db.lookup$size <- size_nb
platypus_url_lookup <- db.lookup
print("--------------")
print("Updating R interface DB lookup list in platpyusdb_lookup bucket")
googleCloudStorageR::gcs_save(platypus_url_lookup,file = "platypus_url_lookup.RData", bucket = "platypusdb_lookup")
print("--------------")
print("Getting updated user lookup table for website")
user.lookup.list <- list()
for(i in 1:length(unique(db.lookup$project_id))){
cur.db.lookup <- subset(db.lookup, project_id == unique(db.lookup$project_id)[i])
print(paste0("Processing project ", i , " of ", length(unique(db.lookup$project_id))))
cols <- c("Project ID", "Title", "Organism", "Celltypes", "n","Authors", "DOI", "Journal", "Date", "VDJ_Raw.zip","GEX_Raw.zip")
user.lookup.cur <- stats::setNames(data.frame(matrix(ncol = length(cols), nrow = 1)), cols)
#load in the metadata frame
url_ <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__metadata.RData")
load(url(url_), envir = .GlobalEnv)
cur_meta <- get(paste0(cur.db.lookup$project_id[1], "__metadata"))
rm(list = ls(pattern = paste0(cur.db.lookup$project_id[1], "__metadata"), envir = .GlobalEnv), envir = .GlobalEnv)
#fill in the content (parser for the metadata file)
cur_meta <- cur_meta[c(1:2),] #We only need the two first rows
user.lookup.cur[,"Project ID"] <- cur.db.lookup$project_id[1]
user.lookup.cur[,"Title"] <- cur_meta[2,1]
user.lookup.cur[,"Organism"] <- cur_meta[2,2]
user.lookup.cur[,"Celltypes"] <- cur_meta[2,3]
user.lookup.cur[,"n"] <- cur_meta[2,4]
user.lookup.cur[,"Authors"] <- cur_meta[2,5]
user.lookup.cur[,"Date"] <- cur_meta[2,6]
user.lookup.cur[,"Journal"] <- cur_meta[2,7]
user.lookup.cur[,"DOI"] <- cur_meta[2,8]
user.lookup.cur[,"VDJ_Raw.zip"] <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__VDJ_RAW.zip")
user.lookup.cur[,"GEX_Raw.zip"] <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__GEX_RAW.zip")
user.lookup.list[[i]] <- user.lookup.cur
}
print("--------------")
print("Saving PlatypusDB_content_table.csv to the current working directory. Please push this to Githup to update the PlatypusDB website")
user.lookup.list <- as.data.frame(dplyr::bind_rows(user.lookup.list))
utils::write.csv(user.lookup.list, file = "PlatypusDB_content_table.csv", row.names = F)
print(paste0(Sys.time()," Done"))
return(list("Lookup for R interface" = platypus_url_lookup, "User lookup for website" = user.lookup.list))
}
#
PlatypusDB_refresh_lookup()
#' This function generates new db lookup and user lookup tables from the PlatypusDB. This is normally done within the Platypus_DB_upload function. In case of manual changes to files, incomplete uploads or other changes after the last use of PlatypusDB_upload, this function may be used to generate up-to-date lookup tables. To use, you will need writing permission from this R session
#'@param bucket.name Character. Name of bucket containing data in. Defaults to "platypusdb_data" (This is a feature to support future changes to the database)
#' @return Writes a .csv file to the current working directory, which contains the lookup table for the website. The lookup for the R interface will be uploaded to PlatypusDB and returned for control
#' @export
#' @examples
#' \dontrun{
#' new_table <- PlatypusDB_refresh_lookup(bucket.name = "bucket with database data")
#' }
#'
PlatypusDB_refresh_lookup <- function(bucket.name){
platypusdb_lookup <- NULL
project.id <- NULL
if(missing(bucket.name)) bucket.name <- "platypusdb_data"
print("--------------")
print("Getting updated DB lookup list")
#get an updated list of the projects with direct download links (main db lookup table)
db.lookup <- googleCloudStorageR::gcs_list_objects(bucket = bucket.name)
db.lookup$project_id <- stringr::str_split(db.lookup$name, "_", simplify = T)[,1]
db.lookup$sample_id <- stringr::str_split(db.lookup$name, "_", simplify = T)[,2]
db.lookup$filetype <- stringr::str_split(db.lookup$name, "_", simplify = T)[,3]
db.lookup$url <- "none"
for(j in 1:nrow(db.lookup)){
db.lookup$url[j] <- paste0("https://storage.googleapis.com/",bucket.name,"/", db.lookup$name[j]) #append url download info to each entry
}
for(k in 1:ncol(db.lookup)){
db.lookup[,k] <- as.character(db.lookup[,k])
}
#adjust size column to make everything in bytes
size_nb <- as.numeric(stringr::str_split(db.lookup$size, " ", simplify = T)[,1])
size_id <- stringr::str_split(db.lookup$size, " ", simplify = T)[,2]
size_id <- gsub("bytes","1", size_id)
size_id <- gsub("Kb","1000", size_id)
size_id <- gsub("Mb","1000000", size_id)
size_id <- gsub("Gb","1000000000", size_id)
size_id <- as.numeric(size_id)
size_nb <- size_nb * size_id
db.lookup$size <- size_nb
platypus_url_lookup <- db.lookup
print("--------------")
print("Updating R interface DB lookup list in platpyusdb_lookup bucket")
googleCloudStorageR::gcs_save(platypus_url_lookup,file = "platypus_url_lookup.RData", bucket = "platypusdb_lookup")
print("--------------")
print("Getting updated user lookup table for website")
user.lookup.list <- list()
for(i in 1:length(unique(db.lookup$project_id))){
cur.db.lookup <- subset(db.lookup, project_id == unique(db.lookup$project_id)[i])
print(paste0("Processing project ", i , " of ", length(unique(db.lookup$project_id))))
cols <- c("Project ID", "Title", "Organism", "Celltypes", "n","Authors", "DOI", "Journal", "Date", "VDJ_Raw.zip","GEX_Raw.zip")
user.lookup.cur <- stats::setNames(data.frame(matrix(ncol = length(cols), nrow = 1)), cols)
#load in the metadata frame
url_ <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__metadata.RData")
load(url(url_), envir = .GlobalEnv)
cur_meta <- get(paste0(cur.db.lookup$project_id[1], "__metadata"))
rm(list = ls(pattern = paste0(cur.db.lookup$project_id[1], "__metadata"), envir = .GlobalEnv), envir = .GlobalEnv)
#fill in the content (parser for the metadata file)
cur_meta <- cur_meta[c(1:2),] #We only need the two first rows
user.lookup.cur[,"Project ID"] <- cur.db.lookup$project_id[1]
user.lookup.cur[,"Title"] <- cur_meta[2,1]
user.lookup.cur[,"Organism"] <- cur_meta[2,2]
user.lookup.cur[,"Celltypes"] <- cur_meta[2,3]
user.lookup.cur[,"n"] <- cur_meta[2,4]
user.lookup.cur[,"Authors"] <- cur_meta[2,5]
user.lookup.cur[,"Date"] <- cur_meta[2,6]
user.lookup.cur[,"Journal"] <- cur_meta[2,7]
user.lookup.cur[,"DOI"] <- cur_meta[2,8]
user.lookup.cur[,"VDJ_Raw.zip"] <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__VDJ_RAW.zip")
user.lookup.cur[,"GEX_Raw.zip"] <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__GEX_RAW.zip")
user.lookup.list[[i]] <- user.lookup.cur
}
print("--------------")
print("Saving PlatypusDB_content_table.csv to the current working directory. Please push this to Githup to update the PlatypusDB website")
user.lookup.list <- as.data.frame(dplyr::bind_rows(user.lookup.list))
utils::write.csv(user.lookup.list, file = "PlatypusDB_content_table.csv", row.names = F)
print(paste0(Sys.time()," Done"))
return(list("Lookup for R interface" = platypus_url_lookup, "User lookup for website" = user.lookup.list))
}
#
PlatypusDB_refresh_lookup()
url_ <- paste0("https://storage.googleapis.com/", "platypusdb_data","/", "kreiner2021a", "__metadata.RData")
load(url(url_), envir = .GlobalEnv)
View(kreiner2021a__metadata)
load(url("https://storage.googleapis.com/platypusdb_data/kreiner2021a__metadata.RData"))
View(kreiner2021a__metadata)
load("C:/Users/vickr/Downloads/kreiner2021a__metadata.gz")
View(kreiner2021a__metadata)
load("C:/Users/vickr/Downloads/kreiner2021a__metadata (1).gz")
View(kreiner2021a__metadata)
load(url("https://storage.googleapis.com/platypusdb_data/kreiner2021a__metadata.RData"))
View(kreiner2021a__metadata)
usethis::use_github_action_check_standard()
install.packages("badgecreatr")
badgecreatr::badge_license()
url_ <- paste0("https://storage.googleapis.com/", "platypusdb_data","/", "kreiner2021a", "__metadata.RData")
load(url(url_), envir = .GlobalEnv)
View(kreiner2021a__metadata)
#' This function generates new db lookup and user lookup tables from the PlatypusDB. This is normally done within the Platypus_DB_upload function. In case of manual changes to files, incomplete uploads or other changes after the last use of PlatypusDB_upload, this function may be used to generate up-to-date lookup tables. To use, you will need writing permission from this R session
#'@param bucket.name Character. Name of bucket containing data in. Defaults to "platypusdb_data" (This is a feature to support future changes to the database)
#' @return Writes a .csv file to the current working directory, which contains the lookup table for the website. The lookup for the R interface will be uploaded to PlatypusDB and returned for control
#' @export
#' @examples
#' \dontrun{
#' new_table <- PlatypusDB_refresh_lookup(bucket.name = "bucket with database data")
#' }
#'
PlatypusDB_refresh_lookup <- function(bucket.name){
platypusdb_lookup <- NULL
project.id <- NULL
if(missing(bucket.name)) bucket.name <- "platypusdb_data"
print("--------------")
print("Getting updated DB lookup list")
#get an updated list of the projects with direct download links (main db lookup table)
db.lookup <- googleCloudStorageR::gcs_list_objects(bucket = bucket.name)
db.lookup$project_id <- stringr::str_split(db.lookup$name, "_", simplify = T)[,1]
db.lookup$sample_id <- stringr::str_split(db.lookup$name, "_", simplify = T)[,2]
db.lookup$filetype <- stringr::str_split(db.lookup$name, "_", simplify = T)[,3]
db.lookup$url <- "none"
for(j in 1:nrow(db.lookup)){
db.lookup$url[j] <- paste0("https://storage.googleapis.com/",bucket.name,"/", db.lookup$name[j]) #append url download info to each entry
}
for(k in 1:ncol(db.lookup)){
db.lookup[,k] <- as.character(db.lookup[,k])
}
#adjust size column to make everything in bytes
size_nb <- as.numeric(stringr::str_split(db.lookup$size, " ", simplify = T)[,1])
size_id <- stringr::str_split(db.lookup$size, " ", simplify = T)[,2]
size_id <- gsub("bytes","1", size_id)
size_id <- gsub("Kb","1000", size_id)
size_id <- gsub("Mb","1000000", size_id)
size_id <- gsub("Gb","1000000000", size_id)
size_id <- as.numeric(size_id)
size_nb <- size_nb * size_id
db.lookup$size <- size_nb
platypus_url_lookup <- db.lookup
print("--------------")
print("Updating R interface DB lookup list in platpyusdb_lookup bucket")
googleCloudStorageR::gcs_save(platypus_url_lookup,file = "platypus_url_lookup.RData", bucket = "platypusdb_lookup")
print("--------------")
print("Getting updated user lookup table for website")
user.lookup.list <- list()
for(i in 1:length(unique(db.lookup$project_id))){
cur.db.lookup <- subset(db.lookup, project_id == unique(db.lookup$project_id)[i])
print(paste0("Processing project ", i , " of ", length(unique(db.lookup$project_id))))
cols <- c("Project ID", "Title", "Organism", "Celltypes", "n","Authors", "DOI", "Journal", "Date", "VDJ_Raw.zip","GEX_Raw.zip")
user.lookup.cur <- stats::setNames(data.frame(matrix(ncol = length(cols), nrow = 1)), cols)
#load in the metadata frame
url_ <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__metadata.RData")
load(url(url_), envir = .GlobalEnv)
cur_meta <- get(paste0(cur.db.lookup$project_id[1], "__metadata"))
rm(list = ls(pattern = paste0(cur.db.lookup$project_id[1], "__metadata"), envir = .GlobalEnv), envir = .GlobalEnv)
#fill in the content (parser for the metadata file)
cur_meta <- cur_meta[c(1:2),] #We only need the two first rows
user.lookup.cur[,"Project ID"] <- cur.db.lookup$project_id[1]
user.lookup.cur[,"Title"] <- cur_meta[2,1]
user.lookup.cur[,"Organism"] <- cur_meta[2,2]
user.lookup.cur[,"Celltypes"] <- cur_meta[2,3]
user.lookup.cur[,"n"] <- cur_meta[2,4]
user.lookup.cur[,"Authors"] <- cur_meta[2,5]
user.lookup.cur[,"Date"] <- cur_meta[2,6]
user.lookup.cur[,"Journal"] <- cur_meta[2,7]
user.lookup.cur[,"DOI"] <- cur_meta[2,8]
user.lookup.cur[,"VDJ_Raw.zip"] <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__VDJ_RAW.zip")
user.lookup.cur[,"GEX_Raw.zip"] <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__GEX_RAW.zip")
user.lookup.list[[i]] <- user.lookup.cur
}
print("--------------")
print("Saving PlatypusDB_content_table.csv to the current working directory. Please push this to Githup to update the PlatypusDB website")
user.lookup.list <- as.data.frame(dplyr::bind_rows(user.lookup.list))
utils::write.csv(user.lookup.list, file = "PlatypusDB_content_table.csv", row.names = F)
print(paste0(Sys.time()," Done"))
return(list("Lookup for R interface" = platypus_url_lookup, "User lookup for website" = user.lookup.list))
}
#
PlatypusDB_refresh_lookup()
hentication
library(googleCloudStorageR)
library(googleAuthR)
#Set working directory to source file location => this is where the .json authentication file is located
setwd("C:/Dokumente usw/Master/Reddy Lab/1_thesis/3_code/4_DB project/GCS setup")
gcs_auth(json_file = "platypusdb-14f99cb3ceb0.json")
#
PlatypusDB_refresh_lookup()
getwd()
usethis::use_git()
usethis::use_github_action_check_standard()
usethis::use_github_action("check-release")
usethis::use_github_action("check-standard")
devtools::document(roclets = c('rd', 'collate', 'namespace')
)
pkgdown::build_home()
pkgdown::build_home()
