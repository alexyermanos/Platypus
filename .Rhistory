utils::download.file(to_download$url[i], destfile = paste0(path.to.save,curr_download_name, ".RData")) #Saving directly to disk to avoid RAM usage
} else { #Save to disk == F
load(url(to_download$url[i]), envir = .GlobalEnv) #download and load to global enviroment
if(load.to.list == T){
out.list[[i]] <- get(curr_download_name) #if a list of objects is to be returned, add the just loaded object into a list
names(out.list)[i] <- curr_download_name
} else {
out.list[[i]] <- curr_download_name #if not list of objects is to be returned, only the name of the loaded object is appended and will be returned for reference. (see also last few lines of the function)
}
if(load.to.enviroment == F){ #if objects should not present in the global enviroment at the end of the function run, we delete them from the .GlobalEnv again.
rm(list = ls(pattern = curr_download_name, envir = .GlobalEnv), envir = .GlobalEnv)
}
}
print(paste0("Done with ", curr_download_name,"!"))
}, error=function(e){
print(e)
print(paste0("Failed to load",  to_download$url[i]))})
}
} else if(combine.objects == T){ #IF COMBINING: iterating over download groups
for(j in 1:length(unique(to_download$group))){
curr_to_download <- subset(to_download, group == unique(to_download$group)[j])
#get the order right => VDJ first, GEX second
if(nrow(curr_to_download) == 2 & (curr_to_download$filetype[1] %in% c("VDJ.RData", "GEX.RData") | curr_to_download$filetype[1] %in% c("VDJmatrix.RData", "GEXmatrix.RData"))){
curr_to_download <- curr_to_download[c(2,1),] #swap rows
}
tryCatch({
if(nrow(curr_to_download) == 2 & (curr_to_download$filetype[1] %in% c("VDJ.RData", "GEX.RData")| curr_to_download$filetype[1] %in% c("VDJmatrix.RData", "GEXmatrix.RData"))){
cur.out.list <- list()
curr_download_names <- gsub("\\.RData","",curr_to_download$name) #getting the names of the objects. [1] is VDJ [2] is GEX
for(i in 1:nrow(curr_to_download)){ #load both objects!
print(paste0(Sys.time(), ": Starting download of ", curr_to_download$name[i],"..."))
load(url(curr_to_download$url[i]), envir = .GlobalEnv) #download
} #End of loop. Now both files which are to be combined (are part of one predefined group) should be present in the R enviroment
#Combine the objects by parsing. This saves ram because we do not have to copy the GEX object into a new list
cmd <- paste0(curr_download_names[2],"[[1]] <- ", curr_download_names[1], "[[1]]")
try(eval(parse(text=cmd)))
#remove the VDJ object
rm(list = ls(pattern = curr_download_names[1], envir = .GlobalEnv), envir = .GlobalEnv)
#rename the R object
if(stringr::str_detect(curr_to_download$filetype[1], "matrix")){ #rename if a full VGM is being downloaded
comb_download_name <- paste0(curr_to_download$project_id[1], "__VDJGEXmatrix")
eval(parse(text = "assign(comb_download_name, get(curr_download_names[2]), envir = .GlobalEnv)"))
rm(list = ls(pattern = curr_download_names[2], envir = .GlobalEnv), envir = .GlobalEnv)
} else { #rename if RData is being downloaded
comb_download_name <- paste0(curr_to_download$project_id[1], "_", curr_to_download$sample_id[1] , "_VDJGEXdata")
eval(parse(text = "assign(comb_download_name, get(curr_download_names[2]), envir = .GlobalEnv)"))
rm(list = ls(pattern = curr_download_names[2], envir = .GlobalEnv), envir = .GlobalEnv)
}
#We now have the combined download objects named comb_download_name in the global enviroment. The other objects have been deleted.
#Therefore we can proceed with the returning / saving modes as above.
if(load.to.list == T){
out.list[[j]] <- get(comb_download_name)
names(out.list)[j] <- comb_download_name
} else {
out.list[[j]] <- comb_download_name
}
if(save.to.disk == T){
print(paste0("Saving ", curr_download_name," to disk..."))
utils::download.file(to_download$url[i], destfile = paste0(path.to.save,curr_download_name, ".RData")) #Saving directly to disk to avoid RAM usage
}
if(load.to.enviroment == F){
rm(list = ls(pattern = comb_download_name, envir = .GlobalEnv), envir = .GlobalEnv)
}
#Done
print(paste0("Done with ", comb_download_name,"!"))
#This else if is there to catch an instance were the input PlatypusDB paths asked for both two files which should be combined, as well as extra files which are not to be combined (e.g. metadata). Here we can proceed as if combine.objects == F
} else if(nrow(curr_to_download) == 1){ #if the group number of that entry was unique
curr_download_name <- gsub("\\.RData","",curr_to_download$name[1])
print(paste0(Sys.time(), ": Starting download of ", curr_to_download$name[1],"..."))
load(url(curr_to_download$url[1]), envir = .GlobalEnv) #download and load to global enviroment
if(load.to.list == T){
out.list[[j]] <- get(curr_download_name) #if a list of objects is to be returned, add the just loaded object into a list
names(out.list)[j] <- curr_download_name
} else {
out.list[[j]] <- curr_download_name
}
if(save.to.disk == T){
print(paste0("Saving ", curr_download_name," to disk..."))
utils::download.file(to_download$url[i], destfile = paste0(path.to.save,curr_download_name, ".RData")) #Saving directly to disk to avoid RAM usage
}
if(load.to.enviroment == F){
rm(list = ls(pattern = curr_download_name, envir = .GlobalEnv), envir = .GlobalEnv)
}
print(paste0("Done with ", curr_download_name,"!"))
#end of else if(nrow(curr_to_download) == 1)
} else if(nrow(curr_to_download) > 2){
print("Downloads of different samples are not combined. Please set load.to.list = T to return a single list containing info of all downloaded samples which can be used as input to the VDJ_GEX_matrix function")
}
}, error=function(e){
print(e)
print(paste0("Failed to load",  names(out.list)[j]))})
} #end of loop over files
} #end of if(combine.object == T)
print(paste0(Sys.time(), ": Done"))
if(load.to.list == F){
print("Returning names of loaded objects")
return(unlist(out.list)) #unlist to return a simple array of names of loaded files
} else{
print("Returning list of loaded objects")
return(out.list) #return loaded files
}
}
a <- PlatypusDB_fetch("kreiner2021a//metadata")
a <- PlatypusDB_fetch("kreiner2021a//metadata", load.to.list = T)
View(a)
load(url("https://storage.googleapis.com/platypusdb_data/kreiner2021a__metadata.RData"))
View(kreiner2021a__metadata)
load("C:/Users/vickr/Downloads/kreiner2021a__metadata.gz")
View(kreiner2021a__metadata)
load(url("https://storage.googleapis.com/platypusdb_data/kreiner2021a__metadata.RData"))
View(kreiner2021a__metadata)
load("C:/Users/vickr/Downloads/kreiner2021a__metadata.gz")
View(kreiner2021a__metadata)
#' This function generates new db lookup and user lookup tables from the PlatypusDB. This is normally done within the Platypus_DB_upload function. In case of manual changes to files, incomplete uploads or other changes after the last use of PlatypusDB_upload, this function may be used to generate up-to-date lookup tables. To use, you will need writing permission from this R session
#'@param bucket.name Character. Name of bucket containing data in. Defaults to "platypusdb_data" (This is a feature to support future changes to the database)
#' @return Writes a .csv file to the current working directory, which contains the lookup table for the website. The lookup for the R interface will be uploaded to PlatypusDB and returned for control
#' @export
#' @examples
#' \dontrun{
#' new_table <- PlatypusDB_refresh_lookup(bucket.name = "bucket with database data")
#' }
#'
PlatypusDB_refresh_lookup <- function(bucket.name){
platypusdb_lookup <- NULL
project.id <- NULL
if(missing(bucket.name)) bucket.name <- "platypusdb_data"
print("--------------")
print("Getting updated DB lookup list")
#get an updated list of the projects with direct download links (main db lookup table)
db.lookup <- googleCloudStorageR::gcs_list_objects(bucket = bucket.name)
db.lookup$project_id <- stringr::str_split(db.lookup$name, "_", simplify = T)[,1]
db.lookup$sample_id <- stringr::str_split(db.lookup$name, "_", simplify = T)[,2]
db.lookup$filetype <- stringr::str_split(db.lookup$name, "_", simplify = T)[,3]
db.lookup$url <- "none"
for(j in 1:nrow(db.lookup)){
db.lookup$url[j] <- paste0("https://storage.googleapis.com/",bucket.name,"/", db.lookup$name[j]) #append url download info to each entry
}
for(k in 1:ncol(db.lookup)){
db.lookup[,k] <- as.character(db.lookup[,k])
}
#adjust size column to make everything in bytes
size_nb <- as.numeric(stringr::str_split(db.lookup$size, " ", simplify = T)[,1])
size_id <- stringr::str_split(db.lookup$size, " ", simplify = T)[,2]
size_id <- gsub("bytes","1", size_id)
size_id <- gsub("Kb","1000", size_id)
size_id <- gsub("Mb","1000000", size_id)
size_id <- gsub("Gb","1000000000", size_id)
size_id <- as.numeric(size_id)
size_nb <- size_nb * size_id
db.lookup$size <- size_nb
platypus_url_lookup <- db.lookup
print("--------------")
print("Updating R interface DB lookup list in platpyusdb_lookup bucket")
googleCloudStorageR::gcs_save(platypus_url_lookup,file = "platypus_url_lookup.RData", bucket = "platypusdb_lookup")
print("--------------")
print("Getting updated user lookup table for website")
user.lookup.list <- list()
for(i in 1:length(unique(db.lookup$project_id))){
cur.db.lookup <- subset(db.lookup, project_id == unique(db.lookup$project_id)[i])
print(paste0("Processing project ", i , " of ", length(unique(db.lookup$project_id))))
cols <- c("Project ID", "Title", "Organism", "Celltypes", "n","Authors", "DOI", "Journal", "Date", "VDJ_Raw.zip","GEX_Raw.zip")
user.lookup.cur <- stats::setNames(data.frame(matrix(ncol = length(cols), nrow = 1)), cols)
#load in the metadata frame
url_ <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__metadata.RData")
load(url(url_), envir = .GlobalEnv)
cur_meta <- get(paste0(cur.db.lookup$project_id[1], "__metadata"))
rm(list = ls(pattern = paste0(cur.db.lookup$project_id[1], "__metadata")))
#fill in the content (parser for the metadata file)
cur_meta <- cur_meta[c(1:2),] #We only need the two first rows
user.lookup.cur[,"Project ID"] <- cur.db.lookup$project_id[1]
user.lookup.cur[,"Title"] <- cur_meta[2,1]
user.lookup.cur[,"Organism"] <- cur_meta[2,2]
user.lookup.cur[,"Celltypes"] <- cur_meta[2,3]
user.lookup.cur[,"Nr of cells"] <- cur_meta[2,4]
user.lookup.cur[,"Authors"] <- cur_meta[2,5]
user.lookup.cur[,"DOI"] <- cur_meta[2,6]
user.lookup.cur[,"Journal"] <- cur_meta[2,7]
user.lookup.cur[,"Date"] <- cur_meta[2,8]
user.lookup.cur[,"VDJ_Raw.zip"] <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__VDJ_RAW.zip")
user.lookup.cur[,"GEX_Raw.zip"] <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__GEX_RAW.zip")
user.lookup.list[[i]] <- user.lookup.cur
}
print("--------------")
print("Saving PlatypusDB_content_table.csv to the current working directory. Please push this to Githup to update the PlatypusDB website")
user.lookup.list <- as.data.frame(dplyr::bind_rows(user.lookup.list))
utils::write.csv(user.lookup.list, file = "PlatypusDB_content_table.csv", row.names = F)
print(paste0(Sys.time()," Done"))
return(list("Lookup for R interface" = platypus_url_lookup, "User lookup for website" = user.lookup.list))
}
#
PlatypusDB_refresh_lookup()
#' This function generates new db lookup and user lookup tables from the PlatypusDB. This is normally done within the Platypus_DB_upload function. In case of manual changes to files, incomplete uploads or other changes after the last use of PlatypusDB_upload, this function may be used to generate up-to-date lookup tables. To use, you will need writing permission from this R session
#'@param bucket.name Character. Name of bucket containing data in. Defaults to "platypusdb_data" (This is a feature to support future changes to the database)
#' @return Writes a .csv file to the current working directory, which contains the lookup table for the website. The lookup for the R interface will be uploaded to PlatypusDB and returned for control
#' @export
#' @examples
#' \dontrun{
#' new_table <- PlatypusDB_refresh_lookup(bucket.name = "bucket with database data")
#' }
#'
PlatypusDB_refresh_lookup <- function(bucket.name){
platypusdb_lookup <- NULL
project.id <- NULL
if(missing(bucket.name)) bucket.name <- "platypusdb_data"
print("--------------")
print("Getting updated DB lookup list")
#get an updated list of the projects with direct download links (main db lookup table)
db.lookup <- googleCloudStorageR::gcs_list_objects(bucket = bucket.name)
db.lookup$project_id <- stringr::str_split(db.lookup$name, "_", simplify = T)[,1]
db.lookup$sample_id <- stringr::str_split(db.lookup$name, "_", simplify = T)[,2]
db.lookup$filetype <- stringr::str_split(db.lookup$name, "_", simplify = T)[,3]
db.lookup$url <- "none"
for(j in 1:nrow(db.lookup)){
db.lookup$url[j] <- paste0("https://storage.googleapis.com/",bucket.name,"/", db.lookup$name[j]) #append url download info to each entry
}
for(k in 1:ncol(db.lookup)){
db.lookup[,k] <- as.character(db.lookup[,k])
}
#adjust size column to make everything in bytes
size_nb <- as.numeric(stringr::str_split(db.lookup$size, " ", simplify = T)[,1])
size_id <- stringr::str_split(db.lookup$size, " ", simplify = T)[,2]
size_id <- gsub("bytes","1", size_id)
size_id <- gsub("Kb","1000", size_id)
size_id <- gsub("Mb","1000000", size_id)
size_id <- gsub("Gb","1000000000", size_id)
size_id <- as.numeric(size_id)
size_nb <- size_nb * size_id
db.lookup$size <- size_nb
platypus_url_lookup <- db.lookup
print("--------------")
print("Updating R interface DB lookup list in platpyusdb_lookup bucket")
googleCloudStorageR::gcs_save(platypus_url_lookup,file = "platypus_url_lookup.RData", bucket = "platypusdb_lookup")
print("--------------")
print("Getting updated user lookup table for website")
user.lookup.list <- list()
for(i in 1:length(unique(db.lookup$project_id))){
cur.db.lookup <- subset(db.lookup, project_id == unique(db.lookup$project_id)[i])
print(paste0("Processing project ", i , " of ", length(unique(db.lookup$project_id))))
cols <- c("Project ID", "Title", "Organism", "Celltypes", "n","Authors", "DOI", "Journal", "Date", "VDJ_Raw.zip","GEX_Raw.zip")
user.lookup.cur <- stats::setNames(data.frame(matrix(ncol = length(cols), nrow = 1)), cols)
#load in the metadata frame
url_ <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__metadata.RData")
load(url(url_), envir = .GlobalEnv)
cur_meta <- get(paste0(cur.db.lookup$project_id[1], "__metadata"))
rm(list = ls(pattern = paste0(cur.db.lookup$project_id[1], "__metadata")), envir = .GlobalEnv)
#fill in the content (parser for the metadata file)
cur_meta <- cur_meta[c(1:2),] #We only need the two first rows
user.lookup.cur[,"Project ID"] <- cur.db.lookup$project_id[1]
user.lookup.cur[,"Title"] <- cur_meta[2,1]
user.lookup.cur[,"Organism"] <- cur_meta[2,2]
user.lookup.cur[,"Celltypes"] <- cur_meta[2,3]
user.lookup.cur[,"n"] <- cur_meta[2,4]
user.lookup.cur[,"Authors"] <- cur_meta[2,5]
user.lookup.cur[,"Date"] <- cur_meta[2,6]
user.lookup.cur[,"Journal"] <- cur_meta[2,7]
user.lookup.cur[,"DOI"] <- cur_meta[2,8]
user.lookup.cur[,"VDJ_Raw.zip"] <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__VDJ_RAW.zip")
user.lookup.cur[,"GEX_Raw.zip"] <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__GEX_RAW.zip")
user.lookup.list[[i]] <- user.lookup.cur
}
print("--------------")
print("Saving PlatypusDB_content_table.csv to the current working directory. Please push this to Githup to update the PlatypusDB website")
user.lookup.list <- as.data.frame(dplyr::bind_rows(user.lookup.list))
utils::write.csv(user.lookup.list, file = "PlatypusDB_content_table.csv", row.names = F)
print(paste0(Sys.time()," Done"))
return(list("Lookup for R interface" = platypus_url_lookup, "User lookup for website" = user.lookup.list))
}
#' This function generates new db lookup and user lookup tables from the PlatypusDB. This is normally done within the Platypus_DB_upload function. In case of manual changes to files, incomplete uploads or other changes after the last use of PlatypusDB_upload, this function may be used to generate up-to-date lookup tables. To use, you will need writing permission from this R session
#'@param bucket.name Character. Name of bucket containing data in. Defaults to "platypusdb_data" (This is a feature to support future changes to the database)
#' @return Writes a .csv file to the current working directory, which contains the lookup table for the website. The lookup for the R interface will be uploaded to PlatypusDB and returned for control
#' @export
#' @examples
#' \dontrun{
#' new_table <- PlatypusDB_refresh_lookup(bucket.name = "bucket with database data")
#' }
#'
PlatypusDB_refresh_lookup <- function(bucket.name){
platypusdb_lookup <- NULL
project.id <- NULL
if(missing(bucket.name)) bucket.name <- "platypusdb_data"
print("--------------")
print("Getting updated DB lookup list")
#get an updated list of the projects with direct download links (main db lookup table)
db.lookup <- googleCloudStorageR::gcs_list_objects(bucket = bucket.name)
db.lookup$project_id <- stringr::str_split(db.lookup$name, "_", simplify = T)[,1]
db.lookup$sample_id <- stringr::str_split(db.lookup$name, "_", simplify = T)[,2]
db.lookup$filetype <- stringr::str_split(db.lookup$name, "_", simplify = T)[,3]
db.lookup$url <- "none"
for(j in 1:nrow(db.lookup)){
db.lookup$url[j] <- paste0("https://storage.googleapis.com/",bucket.name,"/", db.lookup$name[j]) #append url download info to each entry
}
for(k in 1:ncol(db.lookup)){
db.lookup[,k] <- as.character(db.lookup[,k])
}
#adjust size column to make everything in bytes
size_nb <- as.numeric(stringr::str_split(db.lookup$size, " ", simplify = T)[,1])
size_id <- stringr::str_split(db.lookup$size, " ", simplify = T)[,2]
size_id <- gsub("bytes","1", size_id)
size_id <- gsub("Kb","1000", size_id)
size_id <- gsub("Mb","1000000", size_id)
size_id <- gsub("Gb","1000000000", size_id)
size_id <- as.numeric(size_id)
size_nb <- size_nb * size_id
db.lookup$size <- size_nb
platypus_url_lookup <- db.lookup
print("--------------")
print("Updating R interface DB lookup list in platpyusdb_lookup bucket")
googleCloudStorageR::gcs_save(platypus_url_lookup,file = "platypus_url_lookup.RData", bucket = "platypusdb_lookup")
print("--------------")
print("Getting updated user lookup table for website")
user.lookup.list <- list()
for(i in 1:length(unique(db.lookup$project_id))){
cur.db.lookup <- subset(db.lookup, project_id == unique(db.lookup$project_id)[i])
print(paste0("Processing project ", i , " of ", length(unique(db.lookup$project_id))))
cols <- c("Project ID", "Title", "Organism", "Celltypes", "n","Authors", "DOI", "Journal", "Date", "VDJ_Raw.zip","GEX_Raw.zip")
user.lookup.cur <- stats::setNames(data.frame(matrix(ncol = length(cols), nrow = 1)), cols)
#load in the metadata frame
url_ <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__metadata.RData")
load(url(url_), envir = .GlobalEnv)
cur_meta <- get(paste0(cur.db.lookup$project_id[1], "__metadata"))
rm(list = ls(pattern = paste0(cur.db.lookup$project_id[1], "__metadata")), envir = .GlobalEnv)
#fill in the content (parser for the metadata file)
cur_meta <- cur_meta[c(1:2),] #We only need the two first rows
user.lookup.cur[,"Project ID"] <- cur.db.lookup$project_id[1]
user.lookup.cur[,"Title"] <- cur_meta[2,1]
user.lookup.cur[,"Organism"] <- cur_meta[2,2]
user.lookup.cur[,"Celltypes"] <- cur_meta[2,3]
user.lookup.cur[,"n"] <- cur_meta[2,4]
user.lookup.cur[,"Authors"] <- cur_meta[2,5]
user.lookup.cur[,"Date"] <- cur_meta[2,6]
user.lookup.cur[,"Journal"] <- cur_meta[2,7]
user.lookup.cur[,"DOI"] <- cur_meta[2,8]
user.lookup.cur[,"VDJ_Raw.zip"] <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__VDJ_RAW.zip")
user.lookup.cur[,"GEX_Raw.zip"] <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__GEX_RAW.zip")
user.lookup.list[[i]] <- user.lookup.cur
}
print("--------------")
print("Saving PlatypusDB_content_table.csv to the current working directory. Please push this to Githup to update the PlatypusDB website")
user.lookup.list <- as.data.frame(dplyr::bind_rows(user.lookup.list))
utils::write.csv(user.lookup.list, file = "PlatypusDB_content_table.csv", row.names = F)
print(paste0(Sys.time()," Done"))
return(list("Lookup for R interface" = platypus_url_lookup, "User lookup for website" = user.lookup.list))
}
#
PlatypusDB_refresh_lookup()
#' This function generates new db lookup and user lookup tables from the PlatypusDB. This is normally done within the Platypus_DB_upload function. In case of manual changes to files, incomplete uploads or other changes after the last use of PlatypusDB_upload, this function may be used to generate up-to-date lookup tables. To use, you will need writing permission from this R session
#'@param bucket.name Character. Name of bucket containing data in. Defaults to "platypusdb_data" (This is a feature to support future changes to the database)
#' @return Writes a .csv file to the current working directory, which contains the lookup table for the website. The lookup for the R interface will be uploaded to PlatypusDB and returned for control
#' @export
#' @examples
#' \dontrun{
#' new_table <- PlatypusDB_refresh_lookup(bucket.name = "bucket with database data")
#' }
#'
PlatypusDB_refresh_lookup <- function(bucket.name){
platypusdb_lookup <- NULL
project.id <- NULL
if(missing(bucket.name)) bucket.name <- "platypusdb_data"
print("--------------")
print("Getting updated DB lookup list")
#get an updated list of the projects with direct download links (main db lookup table)
db.lookup <- googleCloudStorageR::gcs_list_objects(bucket = bucket.name)
db.lookup$project_id <- stringr::str_split(db.lookup$name, "_", simplify = T)[,1]
db.lookup$sample_id <- stringr::str_split(db.lookup$name, "_", simplify = T)[,2]
db.lookup$filetype <- stringr::str_split(db.lookup$name, "_", simplify = T)[,3]
db.lookup$url <- "none"
for(j in 1:nrow(db.lookup)){
db.lookup$url[j] <- paste0("https://storage.googleapis.com/",bucket.name,"/", db.lookup$name[j]) #append url download info to each entry
}
for(k in 1:ncol(db.lookup)){
db.lookup[,k] <- as.character(db.lookup[,k])
}
#adjust size column to make everything in bytes
size_nb <- as.numeric(stringr::str_split(db.lookup$size, " ", simplify = T)[,1])
size_id <- stringr::str_split(db.lookup$size, " ", simplify = T)[,2]
size_id <- gsub("bytes","1", size_id)
size_id <- gsub("Kb","1000", size_id)
size_id <- gsub("Mb","1000000", size_id)
size_id <- gsub("Gb","1000000000", size_id)
size_id <- as.numeric(size_id)
size_nb <- size_nb * size_id
db.lookup$size <- size_nb
platypus_url_lookup <- db.lookup
print("--------------")
print("Updating R interface DB lookup list in platpyusdb_lookup bucket")
googleCloudStorageR::gcs_save(platypus_url_lookup,file = "platypus_url_lookup.RData", bucket = "platypusdb_lookup")
print("--------------")
print("Getting updated user lookup table for website")
user.lookup.list <- list()
for(i in 1:length(unique(db.lookup$project_id))){
cur.db.lookup <- subset(db.lookup, project_id == unique(db.lookup$project_id)[i])
print(paste0("Processing project ", i , " of ", length(unique(db.lookup$project_id))))
cols <- c("Project ID", "Title", "Organism", "Celltypes", "n","Authors", "DOI", "Journal", "Date", "VDJ_Raw.zip","GEX_Raw.zip")
user.lookup.cur <- stats::setNames(data.frame(matrix(ncol = length(cols), nrow = 1)), cols)
#load in the metadata frame
url_ <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__metadata.RData")
load(url(url_), envir = .GlobalEnv)
cur_meta <- get(paste0(cur.db.lookup$project_id[1], "__metadata"))
rm(list = ls(pattern = paste0(cur.db.lookup$project_id[1], "__metadata"), envir = .GlobalEnv), envir = .GlobalEnv)
#fill in the content (parser for the metadata file)
cur_meta <- cur_meta[c(1:2),] #We only need the two first rows
user.lookup.cur[,"Project ID"] <- cur.db.lookup$project_id[1]
user.lookup.cur[,"Title"] <- cur_meta[2,1]
user.lookup.cur[,"Organism"] <- cur_meta[2,2]
user.lookup.cur[,"Celltypes"] <- cur_meta[2,3]
user.lookup.cur[,"n"] <- cur_meta[2,4]
user.lookup.cur[,"Authors"] <- cur_meta[2,5]
user.lookup.cur[,"Date"] <- cur_meta[2,6]
user.lookup.cur[,"Journal"] <- cur_meta[2,7]
user.lookup.cur[,"DOI"] <- cur_meta[2,8]
user.lookup.cur[,"VDJ_Raw.zip"] <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__VDJ_RAW.zip")
user.lookup.cur[,"GEX_Raw.zip"] <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__GEX_RAW.zip")
user.lookup.list[[i]] <- user.lookup.cur
}
print("--------------")
print("Saving PlatypusDB_content_table.csv to the current working directory. Please push this to Githup to update the PlatypusDB website")
user.lookup.list <- as.data.frame(dplyr::bind_rows(user.lookup.list))
utils::write.csv(user.lookup.list, file = "PlatypusDB_content_table.csv", row.names = F)
print(paste0(Sys.time()," Done"))
return(list("Lookup for R interface" = platypus_url_lookup, "User lookup for website" = user.lookup.list))
}
#
PlatypusDB_refresh_lookup()
#' This function generates new db lookup and user lookup tables from the PlatypusDB. This is normally done within the Platypus_DB_upload function. In case of manual changes to files, incomplete uploads or other changes after the last use of PlatypusDB_upload, this function may be used to generate up-to-date lookup tables. To use, you will need writing permission from this R session
#'@param bucket.name Character. Name of bucket containing data in. Defaults to "platypusdb_data" (This is a feature to support future changes to the database)
#' @return Writes a .csv file to the current working directory, which contains the lookup table for the website. The lookup for the R interface will be uploaded to PlatypusDB and returned for control
#' @export
#' @examples
#' \dontrun{
#' new_table <- PlatypusDB_refresh_lookup(bucket.name = "bucket with database data")
#' }
#'
PlatypusDB_refresh_lookup <- function(bucket.name){
platypusdb_lookup <- NULL
project.id <- NULL
if(missing(bucket.name)) bucket.name <- "platypusdb_data"
print("--------------")
print("Getting updated DB lookup list")
#get an updated list of the projects with direct download links (main db lookup table)
db.lookup <- googleCloudStorageR::gcs_list_objects(bucket = bucket.name)
db.lookup$project_id <- stringr::str_split(db.lookup$name, "_", simplify = T)[,1]
db.lookup$sample_id <- stringr::str_split(db.lookup$name, "_", simplify = T)[,2]
db.lookup$filetype <- stringr::str_split(db.lookup$name, "_", simplify = T)[,3]
db.lookup$url <- "none"
for(j in 1:nrow(db.lookup)){
db.lookup$url[j] <- paste0("https://storage.googleapis.com/",bucket.name,"/", db.lookup$name[j]) #append url download info to each entry
}
for(k in 1:ncol(db.lookup)){
db.lookup[,k] <- as.character(db.lookup[,k])
}
#adjust size column to make everything in bytes
size_nb <- as.numeric(stringr::str_split(db.lookup$size, " ", simplify = T)[,1])
size_id <- stringr::str_split(db.lookup$size, " ", simplify = T)[,2]
size_id <- gsub("bytes","1", size_id)
size_id <- gsub("Kb","1000", size_id)
size_id <- gsub("Mb","1000000", size_id)
size_id <- gsub("Gb","1000000000", size_id)
size_id <- as.numeric(size_id)
size_nb <- size_nb * size_id
db.lookup$size <- size_nb
platypus_url_lookup <- db.lookup
print("--------------")
print("Updating R interface DB lookup list in platpyusdb_lookup bucket")
googleCloudStorageR::gcs_save(platypus_url_lookup,file = "platypus_url_lookup.RData", bucket = "platypusdb_lookup")
print("--------------")
print("Getting updated user lookup table for website")
user.lookup.list <- list()
for(i in 1:length(unique(db.lookup$project_id))){
cur.db.lookup <- subset(db.lookup, project_id == unique(db.lookup$project_id)[i])
print(paste0("Processing project ", i , " of ", length(unique(db.lookup$project_id))))
cols <- c("Project ID", "Title", "Organism", "Celltypes", "n","Authors", "DOI", "Journal", "Date", "VDJ_Raw.zip","GEX_Raw.zip")
user.lookup.cur <- stats::setNames(data.frame(matrix(ncol = length(cols), nrow = 1)), cols)
#load in the metadata frame
url_ <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__metadata.RData")
load(url(url_), envir = .GlobalEnv)
cur_meta <- get(paste0(cur.db.lookup$project_id[1], "__metadata"))
rm(list = ls(pattern = paste0(cur.db.lookup$project_id[1], "__metadata"), envir = .GlobalEnv), envir = .GlobalEnv)
#fill in the content (parser for the metadata file)
cur_meta <- cur_meta[c(1:2),] #We only need the two first rows
user.lookup.cur[,"Project ID"] <- cur.db.lookup$project_id[1]
user.lookup.cur[,"Title"] <- cur_meta[2,1]
user.lookup.cur[,"Organism"] <- cur_meta[2,2]
user.lookup.cur[,"Celltypes"] <- cur_meta[2,3]
user.lookup.cur[,"n"] <- cur_meta[2,4]
user.lookup.cur[,"Authors"] <- cur_meta[2,5]
user.lookup.cur[,"Date"] <- cur_meta[2,6]
user.lookup.cur[,"Journal"] <- cur_meta[2,7]
user.lookup.cur[,"DOI"] <- cur_meta[2,8]
user.lookup.cur[,"VDJ_Raw.zip"] <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__VDJ_RAW.zip")
user.lookup.cur[,"GEX_Raw.zip"] <- paste0("https://storage.googleapis.com/", bucket.name,"/", cur.db.lookup$project_id[1], "__GEX_RAW.zip")
user.lookup.list[[i]] <- user.lookup.cur
}
print("--------------")
print("Saving PlatypusDB_content_table.csv to the current working directory. Please push this to Githup to update the PlatypusDB website")
user.lookup.list <- as.data.frame(dplyr::bind_rows(user.lookup.list))
utils::write.csv(user.lookup.list, file = "PlatypusDB_content_table.csv", row.names = F)
print(paste0(Sys.time()," Done"))
return(list("Lookup for R interface" = platypus_url_lookup, "User lookup for website" = user.lookup.list))
}
#
PlatypusDB_refresh_lookup()
url_ <- paste0("https://storage.googleapis.com/", "platypusdb_data","/", "kreiner2021a", "__metadata.RData")
load(url(url_), envir = .GlobalEnv)
View(kreiner2021a__metadata)
load(url("https://storage.googleapis.com/platypusdb_data/kreiner2021a__metadata.RData"))
View(kreiner2021a__metadata)
load("C:/Users/vickr/Downloads/kreiner2021a__metadata.gz")
View(kreiner2021a__metadata)
load("C:/Users/vickr/Downloads/kreiner2021a__metadata (1).gz")
View(kreiner2021a__metadata)
load(url("https://storage.googleapis.com/platypusdb_data/kreiner2021a__metadata.RData"))
View(kreiner2021a__metadata)
